# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vpqXMnEauF2CsjJvreXJvpNDgv4iGXWw

# Data Loading and Preparation

In this section, we load the Ames Housing dataset, handle missing values, create new features, and drop the 'ocean_proximity' column.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'
data = pd.read_csv(url)

# Data cleaning and feature engineering
data = data.dropna()
data['rooms_per_household'] = data['total_rooms'] / data['households']
data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']
data['population_per_household'] = data['population'] / data['households']
data = data.drop(columns=['ocean_proximity'])

"""# Data Splitting and Scaling

We split the data into training and testing sets and standardize the features to ensure they have zero mean and unit variance.
"""

# Split data
X = data.drop('median_house_value', axis=1)
y = data['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Model Training

We initialize and train three models: Linear Regression, Random Forest, and Gradient Boosting on the training data.


"""

# Initialize models
lin_reg = LinearRegression()
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train models
lin_reg.fit(X_train_scaled, y_train)
rf_reg.fit(X_train, y_train)
gb_reg.fit(X_train, y_train)

"""# Model Evaluation

We predict house prices on the test data and calculate the Mean Squared Error (MSE) and R² scores for each model. These metrics help us evaluate the performance of the models.
"""

# Predict and evaluate
y_pred_lin = lin_reg.predict(X_test_scaled)
y_pred_rf = rf_reg.predict(X_test)
y_pred_gb = gb_reg.predict(X_test)

mse_lin = mean_squared_error(y_test, y_pred_lin)
mse_rf = mean_squared_error(y_test, y_pred_rf)
mse_gb = mean_squared_error(y_test, y_pred_gb)

r2_lin = r2_score(y_test, y_pred_lin)
r2_rf = r2_score(y_test, y_pred_rf)
r2_gb = r2_score(y_test, y_pred_gb)

print(f'Linear Regression MSE: {mse_lin}, R²: {r2_lin}')
print(f'Random Forest MSE: {mse_rf}, R²: {r2_rf}')
print(f'Gradient Boosting MSE: {mse_gb}, R²: {r2_gb}')

"""# Feature Correlation Heatmap
We visualize the correlation between different features in the dataset using a heatmap. This helps in understanding the relationships between variables and identifying potential multicollinearity.
"""

# Feature Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

"""# Distribution of House Prices

We plot the distribution of house prices to understand the range and frequency of house prices in the dataset. This histogram shows that most houses are priced below  200,000, with a significant number at 500,000, indicating a price ceiling in the dataset.
"""

# Distribution of House Prices
plt.figure(figsize=(10, 6))
sns.histplot(data['median_house_value'], bins=50, kde=True)
plt.title('Distribution of House Prices')
plt.xlabel('House Price')
plt.ylabel('Frequency')
plt.show()

"""# Actual vs Predicted Prices (Gradient Boosting)
We visualize the relationship between actual house prices and the prices predicted by the Gradient Boosting model. Points close to the red line represent accurate predictions.
"""

# For visualization, let's select only two features to make it 2D
X_vis = data[['median_income', 'median_house_value']].dropna()
y_vis = X_vis['median_house_value']
X_vis = X_vis[['median_income']]

# Fit Linear Regression model for visualization
lin_reg_vis = LinearRegression()
lin_reg_vis.fit(X_vis, y_vis)
y_pred_vis = lin_reg_vis.predict(X_vis)

# Plot Linear Regression
plt.figure(figsize=(10, 6))
plt.scatter(X_vis, y_vis, color='blue', label='Data points')
plt.plot(X_vis, y_pred_vis, color='red', linewidth=2, label='Regression line')
plt.title('Linear Regression')
plt.xlabel('Median Income')
plt.ylabel('Median House Value')
plt.legend()
plt.show()

"""# Visualizing Decision Tree (for Random Forest and Gradient Boosting)
We visualize a single decision tree to understand how it splits the data into smaller groups based on feature values. This tree diagram helps in understanding the structure and decision-making process of the tree.
"""

# Fit a single Decision Tree model for visualization
tree_reg_vis = DecisionTreeRegressor(max_depth=3)
tree_reg_vis.fit(X_vis, y_vis)

# Plot Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(tree_reg_vis, filled=True, feature_names=['Median Income'], rounded=True)
plt.title('Decision Tree')
plt.show()

"""# Visualizing Random Forest
We visualize the predictions of a Random Forest model by showing a scatter plot of example data points and their predicted values. Random Forest combines predictions from multiple trees to improve accuracy.
"""

# Fit a Random Forest model for visualization
rf_vis = RandomForestRegressor(n_estimators=5, max_depth=3, random_state=42)
rf_vis.fit(X_vis, y_vis)

# Predict with Random Forest
y_pred_rf_vis = rf_vis.predict(X_vis)

# Plot Random Forest Predictions
plt.figure(figsize=(10, 6))
plt.scatter(X_vis, y_vis, color='blue', label='Data points')
plt.scatter(X_vis, y_pred_rf_vis, color='red', label='RF predictions')
plt.title('Random Forest Predictions')
plt.xlabel('Median Income')
plt.ylabel('Median House Value')
plt.legend()
plt.show()

"""# Visualizing Gradient Boosting
We visualize the sequential improvement in predictions by plotting the predictions of each tree in the Gradient Boosting model. This shows how each tree improves on the errors of the previous ones.
"""

# Fit a Gradient Boosting model for visualization
gb_vis = GradientBoostingRegressor(n_estimators=5, max_depth=3, random_state=42)
gb_vis.fit(X_vis, y_vis)

# Predict with Gradient Boosting
y_pred_gb_vis = list(gb_vis.staged_predict(X_vis))

# Plot Gradient Boosting Sequential Predictions
plt.figure(figsize=(10, 6))
plt.scatter(X_vis, y_vis, color='blue', label='Data points')
for i, y_pred in enumerate(y_pred_gb_vis):
    plt.plot(X_vis, y_pred, label=f'Tree {i+1}')
plt.title('Gradient Boosting Sequential Predictions')
plt.xlabel('Median Income')
plt.ylabel('Median House Value')
plt.legend()
plt.show()

"""# Visualizing Model Performance
We compare the performance of the three models (Linear Regression, Random Forest, and Gradient Boosting) using bar plots for Mean Squared Error (MSE) and R² scores. Lower MSE and higher R² indicate better model performance.
"""

# Visualizing Model Predictions
# Model performance metrics
models = ['Linear Regression', 'Random Forest', 'Gradient Boosting']
mse_values = [mse_lin, mse_rf, mse_gb]
r2_values = [r2_lin, r2_rf, r2_gb]

# Bar plot for Mean Squared Error (MSE)
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(models, mse_values, color=['blue', 'green', 'orange'])
plt.title('Model Comparison - Mean Squared Error (MSE)')
plt.ylabel('MSE (Lower is Better)')
plt.xlabel('Models')
plt.xticks(rotation=15)

# Bar plot for R² Score
plt.subplot(1, 2, 2)
plt.bar(models, r2_values, color=['blue', 'green', 'orange'])
plt.title('Model Comparison - R² Score')
plt.ylabel('R² Score (Higher is Better)')
plt.xlabel('Models')
plt.xticks(rotation=15)

plt.tight_layout()
plt.show()

"""# User Input for House Price Prediction
In this section, we ask the user for details about their house and use the Linear Regression, Random Forest, and Gradient Boosting models to predict the house price based on the user's input.
"""

# Function to predict house price based on user input
def predict_house_price():
    # Input details about the house
    longitude = float(input("Enter longitude: "))
    latitude = float(input("Enter latitude: "))
    housing_median_age = float(input("Enter housing median age: "))
    total_rooms = float(input("Enter total rooms: "))
    total_bedrooms = float(input("Enter total bedrooms: "))
    population = float(input("Enter population: "))
    households = float(input("Enter households: "))
    median_income = float(input("Enter median income: "))

    # Create a DataFrame with the user input
    user_data = pd.DataFrame({
        'longitude': [longitude],
        'latitude': [latitude],
        'housing_median_age': [housing_median_age],
        'total_rooms': [total_rooms],
        'total_bedrooms': [total_bedrooms],
        'population': [population],
        'households': [households],
        'median_income': [median_income],
        'rooms_per_household': [total_rooms / households],
        'bedrooms_per_room': [total_bedrooms / total_rooms],
        'population_per_household': [population / households]
    })

    # Standardize the user input data
    user_data_scaled = scaler.transform(user_data)

    # Predict house price using the three models
    predicted_price_lin = lin_reg.predict(user_data_scaled)
    predicted_price_rf = rf_reg.predict(user_data)
    predicted_price_gb = gb_reg.predict(user_data)

    print(f"The predicted house price using Linear Regression is: ${predicted_price_lin[0]:,.2f}")
    print(f"The predicted house price using Random Forest is: ${predicted_price_rf[0]:,.2f}")
    print(f"The predicted house price using Gradient Boosting is: ${predicted_price_gb[0]:,.2f}")

# Call the function to get user input and predict house price
predict_house_price()

"""# Complete Code"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'
data = pd.read_csv(url)

# Data cleaning and feature engineering
data = data.dropna()
data['rooms_per_household'] = data['total_rooms'] / data['households']
data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']
data = data.drop(columns=['ocean_proximity', 'population', 'households'])

# Split data
X = data.drop('median_house_value', axis=1)
y = data['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
lin_reg = LinearRegression()
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train models
lin_reg.fit(X_train_scaled, y_train)
rf_reg.fit(X_train, y_train)
gb_reg.fit(X_train, y_train)

# Predict and evaluate
y_pred_lin = lin_reg.predict(X_test_scaled)
y_pred_rf = rf_reg.predict(X_test)
y_pred_gb = gb_reg.predict(X_test)

mse_lin = mean_squared_error(y_test, y_pred_lin)
mse_rf = mean_squared_error(y_test, y_pred_rf)
mse_gb = mean_squared_error(y_test, y_pred_gb)

r2_lin = r2_score(y_test, y_pred_lin)
r2_rf = r2_score(y_test, y_pred_rf)
r2_gb = r2_score(y_test, y_pred_gb)

print(f'Linear Regression MSE: {mse_lin}, R²: {r2_lin}')
print(f'Random Forest MSE: {mse_rf}, R²: {r2_rf}')
print(f'Gradient Boosting MSE: {mse_gb}, R²: {r2_gb}')

# Feature Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# Distribution of House Prices
plt.figure(figsize=(10, 6))
sns.histplot(data['median_house_value'], bins=50, kde=True)
plt.title('Distribution of House Prices')
plt.xlabel('House Price')
plt.ylabel('Frequency')
plt.show()

# Actual vs Predicted Prices (Gradient Boosting)
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_gb, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices (Gradient Boosting)')
plt.show()

# Visualizing Model Predictions
# Model performance metrics
models = ['Linear Regression', 'Random Forest', 'Gradient Boosting']
mse_values = [mse_lin, mse_rf, mse_gb]
r2_values = [r2_lin, r2_rf, r2_gb]

# Bar plot for Mean Squared Error (MSE)
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(models, mse_values, color=['blue', 'green', 'orange'])
plt.title('Model Comparison - Mean Squared Error (MSE)')
plt.ylabel('MSE (Lower is Better)')
plt.xlabel('Models')
plt.xticks(rotation=15)

# Bar plot for R² Score
plt.subplot(1, 2, 2)
plt.bar(models, r2_values, color=['blue', 'green', 'orange'])
plt.title('Model Comparison - R² Score')
plt.ylabel('R² Score (Higher is Better)')
plt.xlabel('Models')
plt.xticks(rotation=15)

plt.tight_layout()
plt.show()

# Function to predict house price based on user input
def predict_house_price():
    # Input details about the house
    housing_median_age = float(input("Enter house median age in the area: "))
    total_rooms = float(input("Enter total rooms: "))
    total_bedrooms = float(input("Enter total bedrooms: "))
    median_income = float(input("Enter median income in the area: "))

    # Assign base values to longitude and latitude
    longitude = -120.0
    latitude = 37.0

    # Create a DataFrame with the user input
    user_data = pd.DataFrame({
        'longitude': [longitude],
        'latitude': [latitude],
        'housing_median_age': [housing_median_age],
        'total_rooms': [total_rooms],
        'total_bedrooms': [total_bedrooms],
        'median_income': [median_income],
        'rooms_per_household': [total_rooms / 1],  # Assuming single household for calculation
        'bedrooms_per_room': [total_bedrooms / total_rooms]
    })

    # Standardize the user input data
    user_data_scaled = scaler.transform(user_data)

    # Predict house price using the Random Forest model
    predicted_price_rf = rf_reg.predict(user_data)

    print(f"The predicted house price using Random Forest is: ${predicted_price_rf[0]:,.2f}")

# Call the function to get user input and predict house price
predict_house_price()